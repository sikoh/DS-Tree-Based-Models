{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRfPLX4WgLVJ"
      },
      "source": [
        "# Random Forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRRNhkxcgLVK"
      },
      "source": [
        "- use scikit-learn for **random forests**\n",
        "- do **ordinal encoding** with high-cardinality categoricals\n",
        "- understand how categorical encodings affect trees differently compared to linear models\n",
        "- understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3TH11e1gLVL"
      },
      "source": [
        "Today's lesson has two take-away messages:\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models.\n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5PbOFEuFfGF"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries\n",
        "\n",
        "- **category_encoders**\n",
        "- **graphviz**\n",
        "- ipywidgets\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- seaborn\n",
        "- scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FStAplyRFoEu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-yK8B7gLVW"
      },
      "source": [
        "# Use scikit-learn for random forests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRws7XQhl6P0"
      },
      "source": [
        "## Overview\n",
        "\n",
        "Let's fit a Random Forest!\n",
        "\n",
        "![](https://pbs.twimg.com/media/EGSvKA0UUAEzUZi?format=png)\n",
        "\n",
        "[Chris Albon, MachineLearningFlashcards.com](https://twitter.com/chrisalbon/status/1181261589887909889)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFxMCPSgLVM"
      },
      "source": [
        "### Solution example\n",
        "\n",
        "First, read & wrangle the data.\n",
        "\n",
        "> Define a function to wrangle train, validate, and test sets in the same way. Clean outliers and engineer features. (For example, [what other columns have zeros and shouldn't?](https://github.com/Quartz/bad-data-guide#zeros-replace-missing-values) What other columns are duplicates, or nearly duplicates? Can you extract the year from date_recorded? Can you engineer new features, such as the number of years from waterpump construction to waterpump inspection?)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTLm-rDagLVM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'),\n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20,\n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
        "\n",
        "    # Prevent SettingWithCopyWarning\n",
        "    X = X.copy()\n",
        "\n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "\n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace the zeros with nulls, and impute missing values later.\n",
        "    # Also create a \"missing indicator\" column, because the fact that\n",
        "    # values are missing may be a predictive signal.\n",
        "    cols_with_zeros = ['longitude', 'latitude', 'construction_year',\n",
        "                       'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col+'_MISSING'] = X[col].isnull()\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    duplicates = ['quantity_group', 'payment_type']\n",
        "    X = X.drop(columns=duplicates)\n",
        "\n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "\n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "\n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "\n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "    X['years_MISSING'] = X['years'].isnull()\n",
        "\n",
        "    # return the wrangled dataframe\n",
        "    return X\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2HppBvZgLVP"
      },
      "outputs": [],
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target\n",
        "train_features = train.drop(columns=[target])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists\n",
        "features = numeric_features + categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXmK2brXgLVR"
      },
      "outputs": [],
      "source": [
        "# Arrange data into X features matrix and y target vector\n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ktqsZel6P2"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "[Scikit-Learn User Guide: Random Forests](https://scikit-learn.org/stable/modules/ensemble.html#random-forests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57yyygsdgLVW"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfyk_aa5gLVY"
      },
      "source": [
        "# Do ordinal encoding with high-cardinality categoricals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU4ephmll6P3"
      },
      "source": [
        "## Overview\n",
        "\n",
        "https://contrib.scikit-learn.org/category_encoders/ordinal.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQN89a3Ml6P3"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8d_WJtcgLVZ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs2UPoVdgLVp"
      },
      "source": [
        "# Understand how categorical encodings affect trees differently compared to linear models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UuUZhVul6P3"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8V-A92mgLVp"
      },
      "source": [
        "### Categorical exploration, 1 feature at a time\n",
        "\n",
        "Change `feature`, then re-run these cells!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G35RAzVdgLVq"
      },
      "outputs": [],
      "source": [
        "feature = 'extraction_type_class'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuxHWiH8gLVr"
      },
      "outputs": [],
      "source": [
        "X_train[feature].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVxoC4NngLVt"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(16,9))\n",
        "sns.barplot(\n",
        "    x=train[feature],\n",
        "    y=train['status_group']=='functional',\n",
        "    color='grey'\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w99mek14gLVv"
      },
      "outputs": [],
      "source": [
        "X_train[feature].head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezzK2IdbgLVx"
      },
      "source": [
        "### [One Hot Encoding](https://contrib.scikit-learn.org/category_encoders/onehot.html)\n",
        "\n",
        "> Onehot (or dummy) coding for categorical features, produces one feature per category, each binary.\n",
        "\n",
        "Warning: May run slow, or run out of memory, with high cardinality categoricals!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDQZtV6GgLVy"
      },
      "outputs": [],
      "source": [
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'{len(encoded.columns)} columns')\n",
        "encoded.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ql9Qmw3sNJ7"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mT4A-oDGpOss"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "lr = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbH6wivpsRuV"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6KUluFOqIdK"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dt = make_pipeline(\n",
        "    ce.OneHotEncoder(use_cat_names=True),\n",
        "    SimpleImputer(),\n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yg11_gTsUu6"
      },
      "source": [
        "#### One-Hot Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxHwXGRornNI"
      },
      "outputs": [],
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REZ8HdpsccR"
      },
      "source": [
        "#### One-Hot Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV-grmYKpDp9"
      },
      "outputs": [],
      "source": [
        "# Plot tree\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html\n",
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "\n",
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['onehotencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model,\n",
        "                           out_file=None,\n",
        "                           max_depth=7,\n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_,\n",
        "                           impurity=False,\n",
        "                           filled=True,\n",
        "                           proportion=True,\n",
        "                           rounded=True)\n",
        "display(graphviz.Source(dot_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUd6gzcZgLVz"
      },
      "source": [
        "### [Ordinal Encoding](https://contrib.scikit-learn.org/category_encoders/ordinal.html)\n",
        "\n",
        "> Ordinal encoding uses a single column of integers to represent the classes. An optional mapping dict can be passed in; in this case, we use the knowledge that there is some true order to the classes themselves. Otherwise, the classes are assumed to have no true order and integers are selected at random."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnBz2RbwgLVz"
      },
      "outputs": [],
      "source": [
        "encoder = ce.OrdinalEncoder()\n",
        "encoded = encoder.fit_transform(X_train[[feature]])\n",
        "print(f'1 column, {encoded[feature].nunique()} unique values')\n",
        "encoded.head(20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nd-ZWprasqUM"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ1YpwjvrhfL"
      },
      "outputs": [],
      "source": [
        "lr = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    StandardScaler(),\n",
        "    LogisticRegressionCV(multi_class='auto', solver='lbfgs', cv=5, n_jobs=-1)\n",
        ")\n",
        "\n",
        "lr.fit(X_train[[feature]], y_train)\n",
        "score = lr.score(X_val[[feature]], y_val)\n",
        "print('Logistic Regression, Validation Accuracy', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lO_R3SksuHs"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Validation Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOELD_roriVI"
      },
      "outputs": [],
      "source": [
        "dt = make_pipeline(\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(),\n",
        "    DecisionTreeClassifier(random_state=42)\n",
        ")\n",
        "\n",
        "dt.fit(X_train[[feature]], y_train)\n",
        "score = dt.score(X_val[[feature]], y_val)\n",
        "print('Decision Tree, Validation Accuracy', score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V2zHjiwswTg"
      },
      "source": [
        "#### Ordinal Encoding, Logistic Regression, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9UPYPois8QR"
      },
      "outputs": [],
      "source": [
        "model = lr.named_steps['logisticregressioncv']\n",
        "encoder = lr.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "coefficients = pd.Series(model.coef_[0], encoded_columns)\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvmmvE8fsymh"
      },
      "source": [
        "#### Ordinal Encoding, Decision Tree, Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCvUu4Oms88b"
      },
      "outputs": [],
      "source": [
        "model = dt.named_steps['decisiontreeclassifier']\n",
        "encoder = dt.named_steps['ordinalencoder']\n",
        "encoded_columns = encoder.transform(X_val[[feature]]).columns\n",
        "\n",
        "dot_data = export_graphviz(model,\n",
        "                           out_file=None,\n",
        "                           max_depth=5,\n",
        "                           feature_names=encoded_columns,\n",
        "                           class_names=model.classes_,\n",
        "                           impurity=False,\n",
        "                           filled=True,\n",
        "                           proportion=True,\n",
        "                           rounded=True)\n",
        "display(graphviz.Source(dot_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4EJi2GvgLVa"
      },
      "source": [
        "# Understand how tree ensembles reduce overfitting compared to a single decision tree with unlimited depth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mO_uNNzVl6QG"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nNABF3HgLVg"
      },
      "source": [
        "### What's \"random\" about random forests?\n",
        "1. Each tree trains on a random bootstrap sample of the data. (In scikit-learn, for `RandomForestRegressor` and `RandomForestClassifier`, the `bootstrap` parameter's default is `True`.) This type of ensembling is called Bagging. (Bootstrap AGGregatING.)\n",
        "2. Each split considers a random subset of the features. (In scikit-learn, when the `max_features` parameter is not `None`.)\n",
        "\n",
        "For extra randomness, you can try [\"extremely randomized trees\"](https://scikit-learn.org/stable/modules/ensemble.html#extremely-randomized-trees)!\n",
        "\n",
        ">In extremely randomized trees (see [ExtraTreesClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) and [ExtraTreesRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html) classes), randomness goes one step further in the way splits are computed. As in random forests, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_2cvqZNl6QG"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUYP619CgLVb"
      },
      "source": [
        "### Example: [predicting golf putts](https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/)\n",
        "(1 feature, non-linear, regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4640ukxgLVc"
      },
      "outputs": [],
      "source": [
        "putts = pd.DataFrame(\n",
        "    columns=['distance', 'tries', 'successes'],\n",
        "    data = [[2, 1443, 1346],\n",
        "            [3, 694, 577],\n",
        "            [4, 455, 337],\n",
        "            [5, 353, 208],\n",
        "            [6, 272, 149],\n",
        "            [7, 256, 136],\n",
        "            [8, 240, 111],\n",
        "            [9, 217, 69],\n",
        "            [10, 200, 67],\n",
        "            [11, 237, 75],\n",
        "            [12, 202, 52],\n",
        "            [13, 192, 46],\n",
        "            [14, 174, 54],\n",
        "            [15, 167, 28],\n",
        "            [16, 201, 27],\n",
        "            [17, 195, 31],\n",
        "            [18, 191, 33],\n",
        "            [19, 147, 20],\n",
        "            [20, 152, 24]]\n",
        ")\n",
        "\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0IpCcKggLVd"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "def putt_trees(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeRegressor(max_depth=max_depth),\n",
        "              RandomForestRegressor(max_depth=max_depth, n_estimators=n_estimators)]\n",
        "\n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(putts_X, putts_y)\n",
        "        ax = putts.plot('distance', 'rate of success', kind='scatter', title=name)\n",
        "        ax.step(putts_X, model.predict(putts_X), where='mid')\n",
        "        plt.show()\n",
        "\n",
        "interact(putt_trees, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq4Z_wQ_gLVj"
      },
      "source": [
        "### Go back to Tanzania Waterpumps ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoSE9iT6YXQz"
      },
      "source": [
        "#### Helper function to visualize predicted probabilities\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzIAjGpJgLVj"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import seaborn as sns\n",
        "\n",
        "def pred_heatmap(model, X, features, class_index=-1, title='', num=100):\n",
        "    \"\"\"\n",
        "    Visualize predicted probabilities, for classifier fit on 2 numeric features\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : scikit-learn classifier, already fit\n",
        "    X : pandas dataframe, which was used to fit model\n",
        "    features : list of strings, column names of the 2 numeric features\n",
        "    class_index : integer, index of class label\n",
        "    title : string, title of plot\n",
        "    num : int, number of grid points for each feature\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y_pred_proba : numpy array, predicted probabilities for class_index\n",
        "    \"\"\"\n",
        "    feature1, feature2 = features\n",
        "    min1, max1 = X[feature1].min(), X[feature1].max()\n",
        "    min2, max2 = X[feature2].min(), X[feature2].max()\n",
        "    x1 = np.linspace(min1, max1, num)\n",
        "    x2 = np.linspace(max2, min2, num)\n",
        "    combos = list(itertools.product(x1, x2))\n",
        "    y_pred_proba = model.predict_proba(combos)[:, class_index]\n",
        "    pred_grid = y_pred_proba.reshape(num, num).T\n",
        "    table = pd.DataFrame(pred_grid, columns=x1, index=x2)\n",
        "    sns.heatmap(table, vmin=0, vmax=1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "    return y_pred_proba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiRfPqHjgLVl"
      },
      "source": [
        "### Compare Decision Tree, Random Forest, Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKkMLXhMgLVl"
      },
      "outputs": [],
      "source": [
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'quantity'\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def get_X_y(df, feature1, feature2, target):\n",
        "    features = [feature1, feature2]\n",
        "    X = df[features]\n",
        "    y = df[target]\n",
        "    X = X.fillna(X.median())\n",
        "    X = ce.OrdinalEncoder().fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "def compare_models(max_depth=1, n_estimators=1):\n",
        "    models = [DecisionTreeClassifier(max_depth=max_depth),\n",
        "              RandomForestClassifier(max_depth=max_depth, n_estimators=n_estimators),\n",
        "              LogisticRegression(solver='lbfgs', multi_class='auto')]\n",
        "\n",
        "    for model in models:\n",
        "        name = model.__class__.__name__\n",
        "        model.fit(X, y)\n",
        "        pred_heatmap(model, X, [feature1, feature2], class_index=0, title=name)\n",
        "\n",
        "X, y = get_X_y(train, feature1, feature2, target='status_group')\n",
        "interact(compare_models, max_depth=(1,6,1), n_estimators=(10,40,10));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOQqjLEDgLVn"
      },
      "source": [
        "### Bagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm4aPgs2gLVn"
      },
      "outputs": [],
      "source": [
        "# Do-it-yourself Bagging Ensemble of Decision Trees (like a Random Forest)\n",
        "\n",
        "# Instructions\n",
        "# 1. Choose two features\n",
        "# 2. Run this code cell\n",
        "# 3. Interact with the widget sliders\n",
        "\n",
        "feature1 = 'longitude'\n",
        "feature2 = 'latitude'\n",
        "\n",
        "def waterpumps_bagging(max_depth=1, n_estimators=1):\n",
        "    predicteds = []\n",
        "    for i in range(n_estimators):\n",
        "        title = f'Tree {i+1}'\n",
        "        bootstrap_sample = train.sample(n=len(train), replace=True)\n",
        "        X, y = get_X_y(bootstrap_sample, feature1, feature2, target='status_group')\n",
        "        tree = DecisionTreeClassifier(max_depth=max_depth)\n",
        "        tree.fit(X, y)\n",
        "        predicted = pred_heatmap(tree, X, [feature1, feature2], class_index=0, title=title)\n",
        "        predicteds.append(predicted)\n",
        "\n",
        "    ensembled = np.vstack(predicteds).mean(axis=0)\n",
        "    title = f'Ensemble of {n_estimators} trees, with max_depth={max_depth}'\n",
        "    sns.heatmap(ensembled.reshape(100, 100).T, vmin=0, vmax=1)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.show()\n",
        "\n",
        "interact(waterpumps_bagging, max_depth=(1,6,1), n_estimators=(2,5,1));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYoSBi15akWP"
      },
      "source": [
        "# Review\n",
        "\n",
        "#### Try Tree Ensembles when you do machine learning with labeled, tabular data\n",
        "- \"Tree Ensembles\" means Random Forest or Gradient Boosting models.\n",
        "- [Tree Ensembles often have the best predictive accuracy](https://arxiv.org/abs/1708.05070) with labeled, tabular data.\n",
        "- Why? Because trees can fit non-linear, non-[monotonic](https://en.wikipedia.org/wiki/Monotonic_function) relationships, and [interactions](https://christophm.github.io/interpretable-ml-book/interaction.html) between features.\n",
        "- A single decision tree, grown to unlimited depth, will [overfit](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/). We solve this problem by ensembling trees, with bagging (Random Forest) or boosting (Gradient Boosting).\n",
        "- Random Forest's advantage: may be less sensitive to hyperparameters. Gradient Boosting's advantage: may get better predictive accuracy.\n",
        "\n",
        "#### One-hot encoding isnâ€™t the only way, and may not be the best way, of categorical encoding for tree ensembles.\n",
        "- For example, tree ensembles can work with arbitrary \"ordinal\" encoding! (Randomly assigning an integer to each category.) Compared to one-hot encoding, the dimensionality will be lower, and the predictive accuracy may be just as good or even better.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}