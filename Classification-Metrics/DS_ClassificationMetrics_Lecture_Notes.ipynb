{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sikoh/DS-Tree-Based-Models/blob/main/Classification-Metrics/DS_ClassificationMetrics_Lecture_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMI2k-oBsS08"
      },
      "source": [
        "# Classification Metrics\n",
        "\n",
        "- get and interpret the **confusion matrix** for classification models\n",
        "- use classification metrics: **precision, recall**\n",
        "- understand the relationships between precision, recall, **thresholds, and predicted probabilities**, to help **make decisions and allocate budgets**\n",
        "- Get **ROC AUC** (Receiver Operating Characteristic, Area Under the Curve)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU7RuVcjWdcp"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Run the code cell below. You can work locally (follow the [local setup instructions](https://lambdaschool.github.io/ds/unit2/local/)) or on Colab.\n",
        "\n",
        "Libraries\n",
        "\n",
        "- category_encoders\n",
        "- ipywidgets\n",
        "- matplotlib\n",
        "- numpy\n",
        "- pandas\n",
        "- scikit-learn\n",
        "- seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpFoag9QoTgA"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import sys\n",
        "\n",
        "# If you're on Colab:\n",
        "if 'google.colab' in sys.modules:\n",
        "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
        "    !pip install category_encoders==2.*\n",
        "\n",
        "# If you're working locally:\n",
        "else:\n",
        "    DATA_PATH = '../data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfhziD2Wn_iO"
      },
      "source": [
        "# Get and interpret the confusion matrix for classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HsYd0F1w8LsG"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZNCHldPn_iL"
      },
      "source": [
        "First, load the Tanzania Waterpumps data and fit a model. (This code isn't new, we've seen it all before.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8Cjxzrwn_iL"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "def wrangle(X):\n",
        "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
        "    X = X.copy()\n",
        "\n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "\n",
        "    # Extract components from date_recorded, then drop the original column\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    X['month_recorded'] = X['date_recorded'].dt.month\n",
        "    X['day_recorded'] = X['date_recorded'].dt.day\n",
        "    X = X.drop(columns='date_recorded')\n",
        "\n",
        "    # Engineer feature: how many years from construction_year to date_recorded\n",
        "    X['years'] = X['year_recorded'] - X['construction_year']\n",
        "\n",
        "    # Drop recorded_by (never varies) and id (always varies, random)\n",
        "    unusable_variance = ['recorded_by', 'id']\n",
        "    X = X.drop(columns=unusable_variance)\n",
        "\n",
        "    # Drop duplicate columns\n",
        "    duplicate_columns = ['quantity_group']\n",
        "    X = X.drop(columns=duplicate_columns)\n",
        "\n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these like null values\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, np.nan)\n",
        "\n",
        "    # When columns have zeros and shouldn't, they are like null values\n",
        "    cols_with_zeros = ['construction_year', 'longitude', 'latitude', 'gps_height', 'population']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "\n",
        "    return X\n",
        "\n",
        "\n",
        "# Merge train_features.csv & train_labels.csv\n",
        "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'),\n",
        "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
        "\n",
        "# Read test_features.csv & sample_submission.csv\n",
        "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
        "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
        "\n",
        "# Split train into train & val. Make val the same size as test.\n",
        "target = 'status_group'\n",
        "train, val = train_test_split(train, test_size=len(test),\n",
        "                              stratify=train[target], random_state=42)\n",
        "\n",
        "# Arrange data into X features matrix and y target vector\n",
        "X_train = train.drop(columns=target)\n",
        "y_train = train[target]\n",
        "X_val = val.drop(columns=target)\n",
        "y_val = val[target]\n",
        "X_test = test\n",
        "\n",
        "# Make pipeline!\n",
        "pipeline = make_pipeline(\n",
        "    FunctionTransformer(wrangle, validate=False),\n",
        "    ce.OrdinalEncoder(),\n",
        "    SimpleImputer(strategy='mean'),\n",
        "    RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        ")\n",
        "\n",
        "# Fit on train, score on val\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)\n",
        "print('Validation Accuracy', accuracy_score(y_val, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a0auEXa8LsH"
      },
      "source": [
        "## Follow Along\n",
        "\n",
        "Scikit-learn added a [**`plot_confusion_matrix`**](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_confusion_matrix.html) function in version 0.22!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MSWehj9n_iO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP6FGBGUn_iQ"
      },
      "source": [
        "#### How many correct predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRSaYRPWn_iR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q-3R7Ean_iT"
      },
      "source": [
        "#### How many total predictions were made?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vLAQL05fn_iT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1yQ_jYPn_iV"
      },
      "source": [
        "#### What was the classification accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fskAC6SYn_iW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqFgEm3tn_iY"
      },
      "source": [
        "# Use classification metrics: precision, recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQyia8PK8LsK"
      },
      "source": [
        "## Overview\n",
        "\n",
        "[Scikit-Learn User Guide — Classification Report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGv7OLL4n_iY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1U7HdC6n_ia"
      },
      "source": [
        "#### Wikipedia, [Precision and recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
        "\n",
        "> Both precision and recall are based on an understanding and measure of relevance.\n",
        "\n",
        "> Suppose a computer program for recognizing dogs in photographs identifies 8 dogs in a picture containing 12 dogs and some cats. Of the 8 identified as dogs, 5 actually are dogs (true positives), while the rest are cats (false positives). The program's precision is 5/8 while its recall is 5/12.\n",
        "\n",
        "> High precision means that an algorithm returned substantially more relevant results than irrelevant ones, while high recall means that an algorithm returned most of the relevant results.\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5VTjFCn8LsL"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50R-Xhwdn_ie"
      },
      "source": [
        "#### [We can get precision & recall from the confusion matrix](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIta6Vwsn_if"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY2rfzA4n_ih"
      },
      "source": [
        "#### How many correct predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-anLkCin_ii"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYM6f99cn_ij"
      },
      "source": [
        "#### How many total predictions of \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qCiA8j2n_ik"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXNuZ_Rnn_il"
      },
      "source": [
        "#### What's the precision for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1f7VsyXn_im"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ci4QguAkn_in"
      },
      "source": [
        "#### How many actual \"non functional\" waterpumps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlqxNhlYn_io"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IY-vC-hn_iq"
      },
      "source": [
        "#### What's the recall for \"non functional\"?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4U3v8lPP4KbP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObVED_ugn_is"
      },
      "source": [
        "# Understand the relationships between precision, recall, thresholds, and predicted probabilities, to help make decisions and allocate budgets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrvObAB08LsP"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBcQQJ2kn_is"
      },
      "source": [
        "### Imagine this scenario...\n",
        "\n",
        "Suppose there are over 14,000 waterpumps that you _do_ have some information about, but you _don't_ know whether they are currently functional, or functional but need repair, or non-functional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jEEy86CHn_it"
      },
      "outputs": [],
      "source": [
        "len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3az2llAAn_iu"
      },
      "source": [
        "**You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.** You want to predict, which 2,000 are most likely non-functional or in need of repair, to help you triage and prioritize your waterpump inspections.\n",
        "\n",
        "You have historical inspection data for over 59,000 other waterpumps, which you'll use to fit your predictive model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEWc2zt2n_iv"
      },
      "outputs": [],
      "source": [
        "len(train) + len(val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2LiGJLin_ix"
      },
      "source": [
        "Based on this historical data, if you randomly chose waterpumps to inspect, then about 46% of the waterpumps would need repairs, and 54% would not need repairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JliDXTp5n_iy"
      },
      "outputs": [],
      "source": [
        "y_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dP7fjQJeQEX"
      },
      "outputs": [],
      "source": [
        "random_inspections = 2000\n",
        "print(f'With {random_inspections} random inspections, we expect to repair {0.46*random_inspections} waterpumps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnJ7Fnan_i1"
      },
      "source": [
        "**Can you do better than random at prioritizing inspections?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIh2Xj8fn_i3"
      },
      "source": [
        "In this scenario, we should define our target differently. We want to identify which waterpumps are non-functional _or_ are functional but needs repair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7naqusI0n_i4"
      },
      "outputs": [],
      "source": [
        "y_train = y_train != 'functional'\n",
        "y_val = y_val != 'functional'\n",
        "y_train.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1UR1t8Zn_i6"
      },
      "source": [
        "We already made our validation set the same size as our test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHHIplB7n_i8"
      },
      "outputs": [],
      "source": [
        "len(val) == len(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g41DA70rn_i9"
      },
      "source": [
        "We can refit our model, using the redefined target.\n",
        "\n",
        "Then make predictions for the validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXL0LaXQn_i-"
      },
      "outputs": [],
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6eX8ciI8Lsb"
      },
      "source": [
        "## Follow Along"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qISPzM43n_jA"
      },
      "source": [
        "#### Look at the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y72fakpmn_jB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M30BXR6Rn_jC"
      },
      "source": [
        "#### How many total predictions of \"True\" (\"non functional\" or \"functional needs repair\") ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IeTJFo8n_jD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aZSdskSn_jF"
      },
      "source": [
        "### We don't have \"budget\" to take action on all these predictions\n",
        "\n",
        "- But we can get predicted probabilities, to rank the predictions.\n",
        "- Then change the threshold, to change the number of positive predictions, based on our budget."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXkfXDDZn_jF"
      },
      "source": [
        "### Get predicted probabilities and plot the distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwfe7j7W_jTp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD6pRFKOn_jH"
      },
      "source": [
        "### Change the threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjOhH0BMB55A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_Jq56v8Lsd"
      },
      "source": [
        "### Or, get exactly 2,000 positive predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Enaicj8Lsd"
      },
      "source": [
        "Identify the 2,000 waterpumps in the validation set with highest predicted probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk9ssRW08Lsd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNiwbIsa8Lsd"
      },
      "source": [
        "Most of these top 2,000 waterpumps will be relevant recommendations, meaning `y_val==True`, meaning the waterpump is non-functional or needs repairs.\n",
        "\n",
        "Some of these top 2,000 waterpumps will be irrelevant recommendations, meaning `y_val==False`, meaning the waterpump is functional and does not need repairs.\n",
        "\n",
        "Let's look at a random sample of 50 out of these top 2,000:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZogURUpC8Lse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWjXuKlE8Lse"
      },
      "source": [
        "So how many of our recommendations were relevant? ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTkrNpI-8Lse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GwIqw6S8Lse"
      },
      "source": [
        "What's the precision for this subset of 2,000 predictions?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acr4wrBU8Lse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "top80m_Gn_jI"
      },
      "source": [
        "### In this scenario ...\n",
        "\n",
        "Accuracy _isn't_ the best metric!\n",
        "\n",
        "Instead, change the threshold, to change the number of positive predictions, based on the budget. (You have the time and resources to go to just 2,000 waterpumps for proactive maintenance.)\n",
        "\n",
        "Then, evaluate with the precision for \"non functional\"/\"functional needs repair\".\n",
        "\n",
        "This is conceptually like **Precision@K**, where k=2,000.\n",
        "\n",
        "Read more here: [Recall and Precision at k for Recommender Systems: Detailed Explanation with examples](https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54)\n",
        "\n",
        "> Precision at k is the proportion of recommended items in the top-k set that are relevant\n",
        "\n",
        "> Mathematically precision@k is defined as: `Precision@k = (# of recommended items @k that are relevant) / (# of recommended items @k)`\n",
        "\n",
        "> In the context of recommendation systems we are most likely interested in recommending top-N items to the user. So it makes more sense to compute precision and recall metrics in the first N items instead of all the items. Thus the notion of precision and recall at k where k is a user definable integer that is set by the user to match the top-N recommendations objective.\n",
        "\n",
        "We asked, can you do better than random at prioritizing inspections?\n",
        "\n",
        "If we had randomly chosen waterpumps to inspect, we estimate that only 920 waterpumps would be repaired after 2,000 maintenance visits. (46%)\n",
        "\n",
        "But using our predictive model, in the validation set, we succesfully identified over 1,900 waterpumps in need of repair!\n",
        "\n",
        "So we will use this predictive model with the dataset of over 14,000 waterpumps that we _do_ have some information about, but we _don't_ know whether they are currently functional, or functional but need repair, or non-functional.\n",
        "\n",
        "We will predict which 2,000 are most likely non-functional or in need of repair.\n",
        "\n",
        "We estimate that approximately 1,900 waterpumps will be repaired after these 2,000 maintenance visits.\n",
        "\n",
        "So we're confident that our predictive model will help triage and prioritize waterpump inspections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vExJe5H78Lsf"
      },
      "source": [
        "### But ...\n",
        "\n",
        "This metric (~1,900 waterpumps repaired after 2,000 maintenance visits) is specific for _one_ classification problem and _one_ possible trade-off.\n",
        "\n",
        "Can we get an evaluation metric that is generic for _all_ classification problems and _all_ possible trade-offs?\n",
        "\n",
        "Yes — the most common such metric is **ROC AUC.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIhiJ0CS8Lsf"
      },
      "source": [
        "## Get ROC AUC (Receiver Operating Characteristic, Area Under the Curve)\n",
        "\n",
        "[Wikipedia explains,](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) \"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. **The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.**\"\n",
        "\n",
        "ROC AUC is the area under the ROC curve. [It can be interpreted](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) as \"the expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative.\"\n",
        "\n",
        "ROC AUC measures **how well a classifier ranks predicted probabilities.** So, when you get your classifier’s ROC AUC score, you need to **use predicted probabilities, not discrete predictions.**\n",
        "\n",
        "ROC AUC ranges **from 0 to 1.** Higher is better. A naive majority class **baseline** will have an ROC AUC score of **0.5.**\n",
        "\n",
        "#### Scikit-Learn docs\n",
        "- [User Guide: Receiver operating characteristic (ROC)](https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc)\n",
        "- [sklearn.metrics.roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html)\n",
        "- [sklearn.metrics.roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)\n",
        "\n",
        "#### More links\n",
        "- [ROC curves and Area Under the Curve explained](https://www.dataschool.io/roc-curves-and-auc-explained/)\n",
        "- [The philosophical argument for using ROC curves](https://lukeoakdenrayner.wordpress.com/2018/01/07/the-philosophical-argument-for-using-roc-curves/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlPF1lf_8Lsg"
      },
      "outputs": [],
      "source": [
        "# \"The ROC curve is created by plotting the true positive rate (TPR)\n",
        "# against the false positive rate (FPR)\n",
        "# at various threshold settings.\"\n",
        "\n",
        "# Use scikit-learn to calculate TPR & FPR at various thresholds\n",
        "from sklearn.metrics import roc_curve\n",
        "fpr, tpr, thresholds = roc_curve(y_val, y_pred_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "recHYb1x8Lsg"
      },
      "outputs": [],
      "source": [
        "# See the results in a table\n",
        "pd.DataFrame({\n",
        "    'False Positive Rate': fpr,\n",
        "    'True Positive Rate': tpr,\n",
        "    'Threshold': thresholds\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta2rha2r8Lsg"
      },
      "outputs": [],
      "source": [
        "# See the results on a plot.\n",
        "# This is the \"Receiver Operating Characteristic\" curve\n",
        "plt.scatter(fpr, tpr)\n",
        "plt.title('ROC curve')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D-JtD01n8Lsh"
      },
      "outputs": [],
      "source": [
        "# Use scikit-learn to calculate the area under the curve.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "roc_auc_score(y_val, y_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGd34zV68Lsh"
      },
      "source": [
        "**Recap:** ROC AUC measures how well a classifier ranks predicted probabilities. So, when you get your classifier’s ROC AUC score, you need to use predicted probabilities, not discrete predictions.\n",
        "\n",
        "Your code may look something like this:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import roc_auc_score\n",
        "y_pred_proba = model.predict_proba(X_test_transformed)[:, -1] # Probability for last class\n",
        "print('Test ROC AUC:', roc_auc_score(y_test, y_pred_proba))\n",
        "```\n",
        "\n",
        "ROC AUC ranges from 0 to 1. Higher is better. A naive majority class baseline will have an ROC AUC score of 0.5."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}